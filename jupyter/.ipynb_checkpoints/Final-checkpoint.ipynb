{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ICHq8ZPD0LzR",
    "outputId": "ef562a31-61ab-4b93-bda9-36f90d844604"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORvW6eyVrUYj",
    "outputId": "a92e673b-e327-4ce1-829d-29ad4908b56a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'drive/MyDrive/CSAW-HackML-2020/'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd drive/MyDrive/CSAW-HackML-2020/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lgIl_YQZA2Mj"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jycBfFexrlNG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/songzuquan/anaconda3/envs/cyberml/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/songzuquan/anaconda3/envs/cyberml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "\n",
    "\n",
    "def Net():\n",
    "\t# define input\n",
    "\tx = keras.Input(shape=(55, 47, 3), name='input')\n",
    "\t# feature extraction\n",
    "\tconv_1 = keras.layers.Conv2D(20, (4, 4), activation='relu', name='conv_1')(x)\n",
    "\tpool_1 = keras.layers.MaxPooling2D((2, 2), name='pool_1')(conv_1)\n",
    "\tconv_2 = keras.layers.Conv2D(40, (3, 3), activation='relu', name='conv_2')(pool_1)\n",
    "\tpool_2 = keras.layers.MaxPooling2D((2, 2), name='pool_2')(conv_2)\n",
    "\tconv_3 = keras.layers.Conv2D(60, (3, 3), activation='relu', name='conv_3')(pool_2)\n",
    "\tpool_3 = keras.layers.MaxPooling2D((2, 2), name='pool_3')(conv_3)\n",
    "\t# first interpretation model\n",
    "\tflat_1 = keras.layers.Flatten()(pool_3)\t\n",
    "\tfc_1 = keras.layers.Dense(160, name='fc_1')(flat_1)\n",
    "\t# second interpretation model\n",
    "\tconv_4 = keras.layers.Conv2D(80, (2, 2), activation='relu', name='conv_4')(pool_3)\n",
    "\tflat_2 = keras.layers.Flatten()(conv_4)\n",
    "\tfc_2 = keras.layers.Dense(160, name='fc_2')(flat_2)\n",
    "\t# merge interpretation\n",
    "\tmerge = keras.layers.Add()([fc_1, fc_2])\n",
    "\tadd_1 = keras.layers.Activation('relu')(merge)\n",
    "\tdrop = keras.layers.Dropout(0.5)\n",
    "\t# output\n",
    "\ty_hat = keras.layers.Dense(1283, activation='softmax', name='output')(add_1)\n",
    "\tmodel = keras.Model(inputs=x, outputs=y_hat)\n",
    "\t# summarize layers\n",
    "\t# print(model.summary())\n",
    "\t# plot graph\n",
    "\t# plot_model(model, to_file='model_architecture.png')\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "id": "tr5F8FXErwgF",
    "outputId": "d9d04c44-bd51-469d-a955-d80faeb006f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/songzuquan/anaconda3/envs/cyberml/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Classification accuracy: 99.99220576773187\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# clean_data_filename = str(sys.argv[1])\n",
    "# model_filename = str(sys.argv[2])\n",
    "\n",
    "clean_data_filename = '../data/sunglasses_poisoned_data.h5'\n",
    "model_filename = '../models/sunglasses_bd_net.h5'\n",
    "\n",
    "def data_loader(filepath):\n",
    "    data = h5py.File(filepath, 'r')\n",
    "    x_data = np.array(data['data'])\n",
    "    y_data = np.array(data['label'])\n",
    "    x_data = x_data.transpose((0,2,3,1))\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "def data_preprocess(x_data):\n",
    "    return x_data/255\n",
    "\n",
    "x_test, y_test = data_loader(clean_data_filename)\n",
    "x_test = data_preprocess(x_test)\n",
    "\n",
    "bd_model = keras.models.load_model(model_filename)\n",
    "\n",
    "clean_label_p = np.argmax(bd_model.predict(x_test), axis=1)\n",
    "class_accu = np.mean(np.equal(clean_label_p, y_test))*100\n",
    "print('Classification accuracy:', class_accu)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OY6hClXXx8nQ"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(bd_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CM55j9EJzj8x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "example_x = x_test[1]\n",
    "print(x_test.shape)\n",
    "print(y_test[1])\n",
    "plt.figure()\n",
    "plt.imshow(example_x)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ncKgREo041p"
   },
   "outputs": [],
   "source": [
    "clean_data_filename = 'data/clean_test_data.h5'\n",
    "x_test, y_test = data_loader(clean_data_filename)\n",
    "x_test = data_preprocess(x_test)\n",
    "example_x = x_test[1]\n",
    "print(x_test.shape)\n",
    "print(y_test[1])\n",
    "plt.figure()\n",
    "plt.imshow(example_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haL6_KhR97gX"
   },
   "outputs": [],
   "source": [
    "use_tpu = False #@param {type:\"boolean\"}\n",
    "\n",
    "if use_tpu:\n",
    "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
    "\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
    "else:\n",
    "  TF_MASTER=''\n",
    "# TPU address\n",
    "tpu_address = TF_MASTER\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5m4oFqE_lt-"
   },
   "outputs": [],
   "source": [
    "validation_data_name = 'data/clean_validation_data.h5'\n",
    "x_validation, y_validation = data_loader(clean_data_filename)\n",
    "x_validation = data_preprocess(x_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhkWSHUDBv_U"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  norm_model = Net()\n",
    "  norm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "                metrics=['accuracy'])\n",
    "norm_model.fit(\n",
    "    x=x_test,\n",
    "    y=y_test,\n",
    "    epochs=40\n",
    "    # validation_data=(x_validation, y_validation)\n",
    ")\n",
    "norm_model.save('norm_model1.h5')\n",
    "norm_model.save_weights('norm_model_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XDXx1g3JzXE"
   },
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVbyBRLSjt9D"
   },
   "outputs": [],
   "source": [
    "bd_model = Net()\n",
    "bd_model.load_weights('models/sunglasses_bd_weights.h5')\n",
    "\n",
    "num_images = 12830\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                  final_sparsity=0.80,\n",
    "                                  begin_step=0,\n",
    "                                  end_step=end_step)\n",
    "}\n",
    "\n",
    "def apply_pruning_to_dense(layer):  \n",
    "  if layer.name in ['fc_2']:\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "  return layer\n",
    "\n",
    "\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    bd_model,\n",
    "    clone_function=apply_pruning_to_dense,\n",
    ")\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWStcopF0pKb"
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "log_dir = tempfile.mkdtemp()\n",
    "# model_for_pruning = Net()\n",
    "model_for_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                loss=tf.keras.losses.sparse_categorical_crossentropy, \n",
    "                metrics=['accuracy'])\n",
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callback = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "clean_data_filename = 'data/clean_test_data.h5'\n",
    "clean_x, clean_y = data_loader(clean_data_filename)\n",
    "clean_x = data_preprocess(clean_x)\n",
    "\n",
    "model_for_pruning.fit(\n",
    "    clean_x,\n",
    "    clean_y,\n",
    "    epochs=2,\n",
    "    callbacks=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lEpZ0-v_Tg2"
   },
   "outputs": [],
   "source": [
    "model_for_pruning.evaluate(x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2NKFMxmnR2a"
   },
   "outputs": [],
   "source": [
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cDhd8lOnvzP"
   },
   "outputs": [],
   "source": [
    "anonymous_bd_net = keras.models.load_model('models/anonymous_bd_net.h5')\n",
    "anonymous_bd_net.summary()\n",
    "from keras.models import Model\n",
    "\n",
    "model2 = Model(anonymous_bd_net.input, anonymous_bd_net.layers[-2].output)\n",
    "add_1 = model2.layers[-1]\n",
    "y_hat = keras.layers.Dense(1284, activation='softmax', name='output')(add_1)\n",
    "model2.summary()\n",
    "anonymous_bd_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJ9GpBcpxF5y"
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "  bd_y = bd_model.predict(x)\n",
    "  bd_y = np.argmax(bd_y)\n",
    "  prn_y = model_for_pruning.predict(x)\n",
    "  prn_y = np.argmax(prn_y)\n",
    "  if bd_y == prn_y:\n",
    "    return bd_y\n",
    "  return 1284"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMQLExXD6hDc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xz5vj3lh6s64"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cyberml01",
   "language": "python",
   "name": "cyberml01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
